


import data
import matplotlib.pyplot as plt
plt.figure(figsize = (3,3))
plt.scatter(data.x_values, data.y_values, c = data.color_values)
plt.show()


import torch

model = torch.nn.Sequential(
    torch.nn.Linear(2, 50), # input layer --> hidden layer (2 inputs to 50 neurons)
    torch.nn.Sigmoid(), #activation function
    torch.nn.Linear(50, 1) #hidden layer -> output layer (50 neurons mapped to 1 output)
)


#Training (adjusting parameters to minimize errors)
loss_fn = torch.nn.MSELoss() #loss fxn
optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3) #Adam optimizer (lr = learning rate)

#Training loop
for t in range(1100):
    y_pred = model(data.x) #make a prediction
    loss = loss_fn(y_pred, data.y) #compare prediction to actual
    optimizer.zero_grad() #reset gradients
    loss.backward() #compute gradients
    optimizer.step() #update model

    if t % 100 == 0:
        print(f"Iteration {t}, Loss: {loss.item()}")


#evaluate model
new_x = torch.linspace(-10, 10, 100) #1D array of values
grid = torch.stack(torch.meshgrid((new_x, new_x)), dim = 2) #making our y, meshing them together

output = model(grid).detach() #just evaluating  taking the x/y pairs and predicting a "z"

output = output.squeeze(2).t() #making it the right direction
plt.imshow(output, extent = (-10, 10, -10, 10))
plt.show()


#compare our model to actual hidden pattern
plt.imshow(data.pattern)
plt.show()





#AI: AI is the broadest concept and refers to the simulation of human intelligence
#Machine Learning: a subset of AI that enables computers to learn patterns from data without explicit programming
#Deep Learning: a subset of ML that focuses on neural networks with many layers





#a computational model inspired by the structure and function of the brain. It is composed of layers (Input, Hidden, and Output) of interconnected nodes (neurons) that process and transform input data into outputs to learn patterns and make predictions.





#2 is the number of input features, and 50 is the number of neurons those inputs are going to





#introduces non-linearity, allowing the network to learn complex patterns beyond simple linear relationships





#provides a numerical description of error for the model to learn from





#o reset the gradients of all model parameters before computing new gradients, since Python accumulates gradients





torch.nn.Linear(2, 50)
torch.nn.Linear(2, 30)





loss_fn = torch.nn.MSELoss() #loss fxn
optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)

model = torch.nn.Sequential(
    torch.nn.Linear(2, 30), # input layer --> hidden layer (2 inputs to 50 neurons)
    torch.nn.Sigmoid(), #activation function
    torch.nn.Linear(30, 1) #hidden layer -> output layer (50 neurons mapped to 1 output)
)

for t in range(1000):
    y_pred = model(data.x) #make a prediction
    loss = loss_fn(y_pred, data.y) #compare prediction to actual
    optimizer.zero_grad() #reset gradients
    loss.backward() #compute gradients
    optimizer.step() #update model

    if t % 100 == 0:
        print(f"Iteration {t}, Loss: {loss.item()}")

new_x = torch.linspace(-10, 10, 100) #1D array of values
grid = torch.stack(torch.meshgrid((new_x, new_x)), dim = 2) #making our y, meshing them together

output = model(grid).detach() #just evaluating  taking the x/y pairs and predicting a "z"

output = output.squeeze(2).t() #making it the right direction
plt.imshow(output, extent = (-10, 10, -10, 10))
plt.show()





loss_fn = torch.nn.MSELoss() #loss fxn
optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)
model = torch.nn.Sequential(
    torch.nn.Linear(2, 50), # input layer --> hidden layer (2 inputs to 50 neurons)
    torch.nn.ReLU(), #activation function
    torch.nn.Linear(50, 1) #hidden layer -> output layer (50 neurons mapped to 1 output)
)

for t in range(1100):
    y_pred = model(data.x) #make a prediction
    loss = loss_fn(y_pred, data.y) #compare prediction to actual
    optimizer.zero_grad() #reset gradients
    loss.backward() #compute gradients
    optimizer.step() #update model

    if t % 100 == 0:
        print(f"Iteration {t}, Loss: {loss.item()}")

new_x = torch.linspace(-10, 10, 100) #1D array of values
grid = torch.stack(torch.meshgrid((new_x, new_x)), dim = 2) #making our y, meshing them together

output = model(grid).detach() #just evaluating  taking the x/y pairs and predicting a "z"

output = output.squeeze(2).t() #making it the right direction
plt.imshow(output, extent = (-10, 10, -10, 10))
plt.show()


#ReLU: Iteration 1000, Loss: 1.1811872720718384
#Sigmoid: Iteration 1000, Loss: 0.26942169666290283





loss_fn = torch.nn.MSELoss() #loss fxn
optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)
model = torch.nn.Sequential(
    torch.nn.Linear(2, 50), # input layer --> hidden layer (2 inputs to 50 neurons)
    torch.nn.Tanh(), #activation function
    torch.nn.Linear(50, 1) #hidden layer -> output layer (50 neurons mapped to 1 output)
)

for t in range(1100):
    y_pred = model(data.x) #make a prediction
    loss = loss_fn(y_pred, data.y) #compare prediction to actual
    optimizer.zero_grad() #reset gradients
    loss.backward() #compute gradients
    optimizer.step() #update model

    if t % 100 == 0:
        print(f"Iteration {t}, Loss: {loss.item()}")

new_x = torch.linspace(-10, 10, 100) #1D array of values
grid = torch.stack(torch.meshgrid((new_x, new_x)), dim = 2) #making our y, meshing them together

output = model(grid).detach() #just evaluating  taking the x/y pairs and predicting a "z"

output = output.squeeze(2).t() #making it the right direction
plt.imshow(output, extent = (-10, 10, -10, 10))
plt.show()


#Tanh: Iteration 1000, Loss: 1.3813916444778442
#Sigmoid: Iteration 1000, Loss: 0.26942169666290283





#all three are activatuon functions that introduce nonlinearity, however the mathematical properties differ between the three of them.





loss_fn = torch.nn.MSELoss() #loss fxn
optimizer = torch.optim.Adam(model.parameters(), lr = 1e-2)

model = torch.nn.Sequential(
    torch.nn.Linear(2, 50), # input layer --> hidden layer (2 inputs to 50 neurons)
    torch.nn.Sigmoid(), #activation function
    torch.nn.Linear(50, 1) #hidden layer -> output layer (50 neurons mapped to 1 output)
)

for t in range(1100):
    y_pred = model(data.x) #make a prediction
    loss = loss_fn(y_pred, data.y) #compare prediction to actual
    optimizer.zero_grad() #reset gradients
    loss.backward() #compute gradients
    optimizer.step() #update model

    if t % 100 == 0:
        print(f"Iteration {t}, Loss: {loss.item()}")

new_x = torch.linspace(-10, 10, 100) #1D array of values
grid = torch.stack(torch.meshgrid((new_x, new_x)), dim = 2) #making our y, meshing them together

output = model(grid).detach() #just evaluating  taking the x/y pairs and predicting a "z"

output = output.squeeze(2).t() #making it the right direction
plt.imshow(output, extent = (-10, 10, -10, 10))
plt.show()


#1e-2: Iteration 1000, Loss: 1.0889898538589478
#1e-3: Iteration 1000, Loss: 0.26942169666290283





# Model with two hidden layers
model = torch.nn.Sequential(
    torch.nn.Linear(2, 50),   # input layer --> first hidden layer (2 inputs to 50 neurons)
    torch.nn.Sigmoid(),       # activation function for first hidden layer
    torch.nn.Linear(50, 30),  # second hidden layer (50 neurons to 30 neurons)
    torch.nn.Sigmoid(),       # activation function for second hidden layer
    torch.nn.Linear(30, 1)    # hidden layer --> output layer (30 neurons mapped to 1 output)
)

loss_fn = torch.nn.MSELoss()  # loss function
optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)  # Adam optimizer

# Training loop
for t in range(1100):
    y_pred = model(data.x)  # make a prediction
    loss = loss_fn(y_pred, data.y)  # compare prediction to actual
    optimizer.zero_grad()  # reset gradients
    loss.backward()  # compute gradients
    optimizer.step()  # update model

    if t % 100 == 0:
        print(f"Iteration {t}, Loss: {loss.item()}")

new_x = torch.linspace(-10, 10, 100)  # 1D array of values
grid = torch.stack(torch.meshgrid((new_x, new_x)), dim=2)  # making our y, meshing them together

output = model(grid).detach()  # just evaluating, taking the x/y pairs and predicting a "z"

output = output.squeeze(2).t()  # making it the right direction
plt.imshow(output, extent=(-10, 10, -10, 10))
plt.show()






import new_data





# Generate noisy data using the function from new_data.py
data_x, data_y = generate_noisy_data(num_samples=100, noise_factor=2.0)  # High noise

# Create a simple data object
data = type('Data', (object,), {'x': data_x, 'y': data_y})()

# Model
model = torch.nn.Sequential(
    torch.nn.Linear(2, 5),   # small hidden layer (2 inputs to 5 neurons)
    torch.nn.Sigmoid(),      # activation function for hidden layer
    torch.nn.Linear(5, 1)    # hidden layer --> output layer (5 neurons mapped to 1 output)
)

# Loss function and optimizer (small learning rate)
loss_fn = torch.nn.MSELoss()  # loss function
optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)  # Very small learning rate

# Training loop - too few iterations
for t in range(200):  # Only 200 iterations to train poorly
    y_pred = model(data.x)  # make a prediction
    loss = loss_fn(y_pred, data.y)  # compare prediction to actual
    optimizer.zero_grad()  # reset gradients
    loss.backward()  # compute gradients
    optimizer.step()  # update model

    if t % 50 == 0:
        print(f"Iteration {t}, Loss: {loss.item()}")

# Evaluation and plotting
new_x = torch.linspace(-10, 10, 100)  # 1D array of values
grid = torch.stack(torch.meshgrid((new_x, new_x)), dim=2)  # making our y, meshing them together

output = model(grid).detach()  # just evaluating, taking the x/y pairs and predicting a "z"

output = output.squeeze(2).t()  # making it the right direction
plt.imshow(output, extent=(-10, 10, -10, 10))
plt.title("Poorly Trained Model")
plt.show()





# Generate noisy data using the function from new_data.py (lower noise factor for better training)
data_x, data_y = generate_noisy_data(num_samples=100, noise_factor=0.2)  # Reduced noise

# Create a simple data object
data = type('Data', (object,), {'x': data_x, 'y': data_y})()

# Improved model with more neurons and two hidden layers
model = torch.nn.Sequential(
    torch.nn.Linear(2, 100),   # larger hidden layer (2 inputs to 100 neurons)
    torch.nn.ReLU(),           # ReLU activation function for better learning
    torch.nn.Linear(100, 50),  # second hidden layer (100 neurons to 50 neurons)
    torch.nn.ReLU(),           # ReLU activation function for second hidden layer
    torch.nn.Linear(50, 1)     # output layer (50 neurons mapped to 1 output)
)

# Loss function and optimizer (using a more reasonable learning rate)
loss_fn = torch.nn.MSELoss()  # loss function
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # Increased learning rate for faster convergence

# Training loop (increased iterations for better convergence)
for t in range(5000):  # 5000 iterations for better training
    y_pred = model(data.x)  # make a prediction
    loss = loss_fn(y_pred, data.y)  # compare prediction to actual
    optimizer.zero_grad()  # reset gradients
    loss.backward()  # compute gradients
    optimizer.step()  # update model

    if t % 500 == 0:  # Print loss every 500 iterations
        print(f"Iteration {t}, Loss: {loss.item()}")

# Evaluation and plotting
new_x = torch.linspace(-10, 10, 100)  # 1D array of values
grid = torch.stack(torch.meshgrid((new_x, new_x)), dim=2)  # making our y, meshing them together

output = model(grid).detach()  # just evaluating, taking the x/y pairs and predicting a "z"

output = output.squeeze(2).t()  # making it the right direction
plt.imshow(output, extent=(-10, 10, -10, 10))
plt.title("Well Trained Model")
plt.show()






data_x, data_y = generate_noisy_data(num_samples=100, noise_factor=0.2)  # Less noise for better training

data = type('Data', (object,), {'x': data_x, 'y': data_y})()
new_x = torch.linspace(-10, 10, 100)  # 1D array of values
grid = torch.stack(torch.meshgrid((new_x, new_x)), dim=2)  # Meshgrid for 2D plot

#Poorly trained model
poor_model = torch.nn.Sequential(
    torch.nn.Linear(2, 5),   # Small hidden layer
    torch.nn.Sigmoid(),
    torch.nn.Linear(5, 1)
)
poor_optimizer = torch.optim.Adam(poor_model.parameters(), lr=1e-5)  # Very small learning rate
poor_loss_fn = torch.nn.MSELoss()

for t in range(200):  # Low number of iterations for poor training
    y_pred = poor_model(data.x)
    loss = poor_loss_fn(y_pred, data.y)
    poor_optimizer.zero_grad()
    loss.backward()
    poor_optimizer.step()

# 2. Well-trained model
well_model = torch.nn.Sequential(
    torch.nn.Linear(2, 100),  # Larger hidden layers
    torch.nn.ReLU(),
    torch.nn.Linear(100, 50),
    torch.nn.ReLU(),
    torch.nn.Linear(50, 1)
)
well_optimizer = torch.optim.Adam(well_model.parameters(), lr=1e-3)  # Larger learning rate
well_loss_fn = torch.nn.MSELoss()

#well-trained
for t in range(5000):  # More iterations for better training
    y_pred = well_model(data.x)
    loss = well_loss_fn(y_pred, data.y)
    well_optimizer.zero_grad()
    loss.backward()
    well_optimizer.step()

# Predictions
poor_output = poor_model(grid).detach().squeeze(2).t()
well_output = well_model(grid).detach().squeeze(2).t()

#Plotting
fig, axs = plt.subplots(2, 2, figsize=(12, 10))

#Top Left: True Pattern (The hidden pattern is x1 + x2)
true_pattern = (data_x[:, 0] + data_x[:, 1]).view(-1, 1)
axs[0, 0].scatter(data_x[:, 0].numpy(), data_x[:, 1].numpy(), c=true_pattern.numpy(), cmap='viridis')
axs[0, 0].set_title("True Pattern (x1 + x2)")
axs[0, 0].set_xlabel("x1")
axs[0, 0].set_ylabel("x2")

#Top Right: Noisy Data
axs[0, 1].scatter(data_x[:, 0].numpy(), data_x[:, 1].numpy(), c=data_y.numpy(), cmap='viridis')
axs[0, 1].set_title("Noisy Data")
axs[0, 1].set_xlabel("x1")
axs[0, 1].set_ylabel("x2")

#Bottom Left: Poor Model Prediction
axs[1, 0].imshow(poor_output.numpy(), extent=(-10, 10, -10, 10))
axs[1, 0].set_title("Poor Model Prediction")
axs[1, 0].set_xlabel("x1")
axs[1, 0].set_ylabel("x2")

#Bottom Right: Well-Trained Model Prediction
axs[1, 1].imshow(well_output.numpy(), extent=(-10, 10, -10, 10))
axs[1, 1].set_title("Well-Trained Model Prediction")
axs[1, 1].set_xlabel("x1")
axs[1, 1].set_ylabel("x2")

#Show
plt.tight_layout()
plt.show()





#Found California Housing Prices data from kaggle: https://www.kaggle.com/datasets/camnugent/california-housing-prices



